import json5, re
from dotenv import load_dotenv
from src.Metrics.get_metrics import calculate_metrics, find_most_imformative_points
from src.OpenRouter.openrouter import call_openrouter
from src.Rater.rater import call_rater_llm_prompt, call_rater_llm_meta_prompt
from src.Recommender.rec_prompt import create_recommender_prompt
from src.TopK_Heap.top_k import TopKHeap

load_dotenv()

def clean_response(reply: str) -> dict:
  try:
    start = reply.find('{')
    end = reply.rfind('}')

    if start == -1 or end == -1 or start > end:
      raise ValueError("Invalid reply")

    data = json5.loads(reply[start:end + 1])
    return data

  except Exception as e:
    print(f"Error decoding JSON: {e}")
    print(reply)
    return {}

def process_reply(instruction: str, recommendation : str, heap: TopKHeap, metrics: dict) -> dict:
  """
  Processes the evaluator reply. Pushes the instruction, metrics, and recommendation
  into the TopKHeap, and returns the processed dict.
  :param instruction: The instruction to process.
  :param recommendation: The recommendation to process.
  :param heap: TopKHeap object to maintain top-k prompts
  :param metrics: Dictionary of classification reports per metric (fluency, coherence, etc.)
  :return: Processed dict pushed to heap
  """

  processed = {
    "instruction": instruction.strip(),
    "metrics" : metrics,
    "recommendation": recommendation.strip(),
  }

  heap.push(processed)
  # print(processed)
  return processed

def call_recommender_llm(instruction: str, metrics: dict, top_points: list[dict], file_path: str, reco_llm_name: str, reco_temp: float = 1.0, reco_top_p: float = 0.95) -> str:
  """
  instruction: the instruction generated by RATER_LLM
  metrics: Dictionary of performances of F1, Accuracy etc. per metric
  top_points: List of top-k points with the most loss.
  eval_llm_name: name of the evaluator llm to use
  reco_temp: Recommender LLM temperature (default value is 1)
  reco_top_p: Recommender LLM top-p (default value is 0.95)
  """
  recommender_prompt = create_recommender_prompt(instruction, metrics, top_points=top_points, file_path=file_path)

  recommender_response = call_openrouter(recommender_prompt, reco_llm_name, temperature=reco_temp, top_p=reco_top_p)
  # No need to clean because response desired is already string.
  # eval_reply = clean_response(eval_reply)
  return recommender_response

if __name__ == '__main__':
  # optim_llm_name = "google/gemini-2.0-flash-exp:free"
  rater_llm_name = "meta-llama/llama-3.1-8b-instruct"
  reco_llm_name = "meta-llama/llama-3.1-8b-instruct"
  filepath = "../Dataset/dataset/df_M11_sampled.parquet"

  top_k_prompts = TopKHeap(3)
  instruction = call_rater_llm_meta_prompt(rater_llm_name=rater_llm_name, top_k_prompts=top_k_prompts)
  print(instruction)
  print('=' * 70)

  evals = call_rater_llm_prompt(instruction=instruction, file_path=filepath, num_examples=20, max_workers=10)

  metrics = calculate_metrics(evals)
  print(metrics)
  print(f"Calculated metrics")
  print("=" * 70)

  top_points = find_most_imformative_points(evals, top_k=5)
  print("Found most imformative points")
  print(top_points)

  recommendation = call_recommender_llm(instruction, metrics, reco_llm_name=reco_llm_name, top_points=top_points)
  # print(f"EVAL_LLM: Generating recommendations")
  print(recommendation)
  print('=' * 70)

  processed_reply = process_reply(instruction, recommendation, top_k_prompts, metrics)
  print('=' * 70)
  print(processed_reply)