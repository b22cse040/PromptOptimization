import json5, re
from dotenv import load_dotenv
from src.Metrics.get_metrics import calculate_metrics
from src.OpenRouter.openrouter import call_openrouter
from src.Rater.rater import call_rater_llm_prompt, call_rater_llm_meta_prompt
from src.Recommender.rec_prompt import create_recommender_prompt
from src.TopK_Heap.top_k import TopKHeap

load_dotenv()

def clean_response(reply: str) -> dict:
  try:
    start = reply.find('{')
    end = reply.rfind('}')

    if start == -1 or end == -1 or start > end:
      raise ValueError("Invalid reply")

    data = json5.loads(reply[start:end + 1])
    return data
  except Exception as e:
    print(f"Error decoding JSON: {e}")
    print(reply)
    print(f"Cleaned Reply: {data}")
    return {}

def process_reply(instruction: str, recommendation : str, heap: TopKHeap, metrics: dict) -> dict:
  """
  Processes the evaluator reply. Pushes the instruction, metrics, and recommendation
  into the TopKHeap, and returns the processed dict.
  :param instruction: The instruction to process.
  :param recommendation: The recommendation to process.
  :param heap: TopKHeap object to maintain top-k prompts
  :param metrics: Dictionary of classification reports per metric (fluency, coherence, etc.)
  :return: Processed dict pushed to heap
  """

  processed = {
    "instruction": instruction.strip(),
    "metrics" : metrics,
    "recommendation": recommendation.strip(),
  }

  heap.push(processed)
  # print(processed)
  return processed

def call_recommender_llm(instruction: str, metrics: dict, reco_llm_name: str) -> str:
  """
  instruction: the instruction generated by RATER_LLM
  metrics: Dictionary of performances of F1, Accuracy etc. per metric
  eval_llm_name: name of the evaluator llm to use
  """
  recommender_prompt = create_recommender_prompt(instruction, metrics)

  recommender_response = call_openrouter(recommender_prompt, reco_llm_name)
  # No need to clean because response desired is already string.
  # eval_reply = clean_response(eval_reply)
  return recommender_response

if __name__ == '__main__':
  # optim_llm_name = "google/gemini-2.0-flash-exp:free"
  rater_llm_name = "deepseek/deepseek-r1-0528-qwen3-8b:free"
  reco_llm_name = "deepseek/deepseek-r1-0528-qwen3-8b:free"
  filepath = "../Dataset/dataset/df_M11_sampled.parquet"

  top_k_prompts = TopKHeap(3)
  instruction = call_rater_llm_meta_prompt(rater_llm_name=rater_llm_name, top_k_prompts=top_k_prompts)
  print(instruction)
  print('=' * 70)

  evals = call_rater_llm_prompt(instruction=instruction, file_path=filepath)

  metrics = calculate_metrics(evals)
  print(metrics)
  print(f"Calculated metrics")
  print("=" * 70)

  recommendation = call_recommender_llm(instruction, metrics, reco_llm_name=reco_llm_name)
  # print(f"EVAL_LLM: Generating recommendations")
  print(recommendation)
  print('=' * 70)

  processed_reply = process_reply(instruction, recommendation, top_k_prompts, metrics)
  print('=' * 70)
  print(processed_reply)