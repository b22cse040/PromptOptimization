from src.Rater.rater import call_rater_llm_prompt, call_rater_llm_prompt, \
  call_rater_llm_meta_prompt
from src.TopK_Heap.top_k import TopKHeap
from typing import Dict

_TASK_DESCRIPTION_EVAL = """
  You are an evaluator in a prompt optimization framework aimed at aligning model-predicted scores with expert human annotations.
  
  You will be given:
  - A set of sample points, each containing:
    - The original article or reference.
    - The machine-generated summary.
    - Ground-truth scores (assigned by expert annotators).
    - Predicted scores (generated by the model being optimized).
  
  Each sample is evaluated along the following four metrics, with score values ranging from 1 to 5:
    
  - Relevance: The rating measures how well the summary captures the key points of 
  the article. Consider whether all and only the important aspects are contained in
  the summary.
  - Consistency: The rating measures whether the facts in the summary are consistent 
  with the facts in the original article. Consider whether the summary does
  reproduce all facts accurately and does not make up untrue information.
  - Fluency: The rating measures the quality of individual sentences, are they 
  well-written and grammatically correct. Consider the quality of individual sentences.
  - Coherence: The rating measures the quality of all sentences collectively, to
  fit together and sound naturally. Consider the quality of the summary as a whole. 
  
  You may:
  - Recommend adjustments to metric definitions if misalignment is observed.
  - Suggest that a particular summary has been under- or over-rated in a specific metric.
  - Highlight patterns in errors across multiple samples (e.g., consistently underrating coherence).
  - Propose revised instructions or guiding principles that would help the model better align with expert annotators.
  
  Be specific, grounded in the provided evidence, and focus on actionable improvements.
"""

def create_evaluator_prompt(sample_points: list[Dict[str, str]],
                            optim_llm_response : dict,
                            task_desc : str =_TASK_DESCRIPTION_EVAL) -> str:
  """
  Creates an evaluator prompt
  :param sample_points: Original Sample Points with Ground Truth scores.
  :param optim_llm_response: Optimized LLM response with predicted scores
  :param task_desc: Task Description for the task at hand
  :return: A string containing the prompt
  """
  # instruction: str = optim_llm_response["instruction"]
  sample_points_text = ""
  i = 1
  for point in sample_points:
    sample_points_text += (
      f"Point: {i}\n"
      f"Text: {point['text']}\n"
      f"ground_fluency: {point['ground_fluency']}\n"
      f"ground_coherence: {point['ground_coherence']}\n"
      f"ground_consistency: {point['ground_consistency']}\n"
      f"ground_relevance: {point['ground_relevance']}\n\n"
    )
    i += 1

  sample_points_predicted = optim_llm_response["sample_points"]
  instruction = optim_llm_response["instruction"]
  optim_llm_response_text = ""
  for idx, sample_point in sample_points_predicted.items():
    optim_llm_response_text += (
      f"Point: {idx}\n"
      # f"text: {sample_point['text']}\n"
      # f"human_summary: {sample_point['human_summary']}\n"
      f"machine_summary: {sample_point['machine_summary']}\n"
      f"predicted_fluency: {sample_point['score']['predicted_fluency']}\n"
      f"predicted_coherence: {sample_point['score']['predicted_coherence']}\n"
      f"predicted_consistency: {sample_point['score']['predicted_consistency']}\n"
      f"predicted_relevance: {sample_point['score']['predicted_relevance']}\n\n"
    )

  _EVALUATOR_PROMPT = f"""
    Task description: 
    {task_desc}
    
    The following samples are to be evaluated: 
    {sample_points_text}
    
    The following LLM Samples contain machine summaries and their performance
    across the metrics as predicted by the LLM, keyed by idx. 
    
    {optim_llm_response_text}
    
    The current instruction is: {instruction}
    
    Important: You must respond with STRICT VALID JSON only. 
    - Use double quotes (") for all keys and string values.
    - Do NOT include any explanations, comments or extra text before or after the JSON.
    - Do not include markdown code blocks (like ```json or ```).
    - Escape all internal newlines and quotes in string values if needed.
    
    Important: Do not reference or include ground-truth metric scores (e.g., no 
    “predicted vs. ground truth” comparisons). Focus only on analyzing the predicted 
    scores and providing recommendations based on patterns in those predictions.
    
    Here is the required JSON structure example (strict format):
    {{
      "instruction": "<same string from instruction>",
      "recommendation": "<your suggestion to better align model outputs with human scores>"
    }}
    
    Do not add any commentary, markdown, or explanation. If you include anything else, the system will raise an error.
    Please adhere to the said output.
  """

  return _EVALUATOR_PROMPT

if __name__ == "__main__":
  rater_llm_name = "deepseek/deepseek-r1-0528-qwen3-8b:free"
  file_path = "../Dataset/dataset/df_M11_sampled.parquet"

  top_k_prompts = TopKHeap(3)

  rater_meta_prompt = call_rater_llm_meta_prompt(top_k_prompts, rater_llm_name)
  print(rater_meta_prompt)

  evals = call_rater_llm_prompt(rater_meta_prompt, file_path=file_path, rater_llm_name=rater_llm_name)
  print(evals)

  # evaluator_prompt = create_evaluator_prompt(sample_points, optim_llm_response)
  # print(evaluator_prompt)
  # print('=' * 100)
  # print(len(evaluator_prompt))